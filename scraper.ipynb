{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4b836e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1: https://bina.az/items/all?page=1\n",
      "Found 28 new items on page 1\n",
      "Fetching page 2: https://bina.az/items/all?page=2\n",
      "Found 28 new items on page 2\n",
      "Fetching page 3: https://bina.az/items/all?page=3\n",
      "Found 28 new items on page 3\n",
      "['4959734', '5165167', '5122759', '4802446', '5138486', '5186362', '4113285', '5171230', '4351437', '4939762', '4061219', '4687575', '4839041', '5172433', '5037112', '4707683', '5114032', '4749417', '5094015', '4966017', '5151006', '4986500', '5184519', '5165139', '5038500', '4624278', '4661811', '5097244', '5170034', '5109657', '5111890', '3485086', '4822793', '5185733', '5104486', '4959777', '5153278', '4999301', '5118470', '4901100', '5115965', '4158518', '4884485', '5068771', '4891042', '5060163', '5107114', '3818945', '5144278', '4486588', '4280166', '5110014', '5112825', '4632009', '5181767', '5089925', '5103590', '4983218', '5187275', '3713070', '4038124', '5167908', '4266689', '5181362', '5186086', '5184523', '5168065', '5163744', '5187276', '5186450', '5187274', '5184531', '5179247', '4194193', '5149274', '5147145', '5095087', '5162858', '5145673', '5184396', '5084859', '5119065', '5161334', '5038503']\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "\n",
    "ITEM_URL_BASE = \"https://bina.az/items/\"\n",
    "BASE_URL = \"https://bina.az/items/all\"\n",
    "HOME_FILE = \"data/test.html\"\n",
    "OUTPUT_FILE = \"data/items_data.json\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "def get_unique_item_ids(max_pages=3):\n",
    "    item_ids = set()\n",
    "    page = 1\n",
    "    has_more_pages = True\n",
    "    \n",
    "    while has_more_pages:\n",
    "        if max_pages and page > max_pages:\n",
    "            break\n",
    "            \n",
    "        url = f\"{BASE_URL}?page={page}\"\n",
    "        print(f\"Fetching page {page}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch page {page}: {response.status_code}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Find all item links and extract IDs from hrefs\n",
    "            current_page_ids = set()\n",
    "            for a in soup.select(\"a.item_link\"):  # Adjust selector if needed\n",
    "                href = a.get(\"href\")\n",
    "                if href:\n",
    "                    # Use regex to extract numeric ID from /items/123456 format\n",
    "                    match = re.search(r'/items/(\\d+)', href)\n",
    "                    if match:\n",
    "                        item_id = match.group(1)\n",
    "                        current_page_ids.add(item_id)\n",
    "            \n",
    "            # Check if we found any new items on this page\n",
    "            if not current_page_ids:\n",
    "                print(f\"No items found on page {page}, stopping pagination.\")\n",
    "                has_more_pages = False\n",
    "            else:\n",
    "                new_items = current_page_ids - item_ids\n",
    "                if not new_items:\n",
    "                    print(f\"No new items found on page {page}, stopping pagination.\")\n",
    "                    has_more_pages = False\n",
    "                else:\n",
    "                    print(f\"Found {len(new_items)} new items on page {page}\")\n",
    "                    item_ids.update(current_page_ids)\n",
    "                    page += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page {page}: {str(e)}\")\n",
    "            break\n",
    "\n",
    "    # Optional: Save raw HTML for debugging (only the last page)\n",
    "    with open(HOME_FILE, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    return item_ids\n",
    "\n",
    "# Run the function and print results\n",
    "item_ids = get_unique_item_ids()\n",
    "links = list(item_ids)\n",
    "print(links)\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57aa24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/84] Processing: 4959734\n",
      "[2/84] Processing: 5165167\n",
      "[3/84] Processing: 5122759\n",
      "[4/84] Processing: 4802446\n",
      "[5/84] Processing: 5138486\n",
      "[6/84] Processing: 5186362\n",
      "[7/84] Processing: 4113285\n",
      "[8/84] Processing: 5171230\n",
      "[9/84] Processing: 4351437\n",
      "[10/84] Processing: 4939762\n",
      "[11/84] Processing: 4061219\n",
      "[12/84] Processing: 4687575\n",
      "[13/84] Processing: 4839041\n",
      "[14/84] Processing: 5172433\n",
      "[15/84] Processing: 5037112\n",
      "[16/84] Processing: 4707683\n",
      "[17/84] Processing: 5114032\n",
      "[18/84] Processing: 4749417\n",
      "[19/84] Processing: 5094015\n",
      "[20/84] Processing: 4966017\n",
      "[21/84] Processing: 5151006\n",
      "[22/84] Processing: 4986500\n",
      "[23/84] Processing: 5184519\n",
      "[24/84] Processing: 5165139\n",
      "[25/84] Processing: 5038500\n",
      "[26/84] Processing: 4624278\n",
      "[27/84] Processing: 4661811\n",
      "[28/84] Processing: 5097244\n",
      "[29/84] Processing: 5170034\n",
      "[30/84] Processing: 5109657\n",
      "[31/84] Processing: 5111890\n",
      "[32/84] Processing: 3485086\n",
      "[33/84] Processing: 4822793\n",
      "[34/84] Processing: 5185733\n",
      "[35/84] Processing: 5104486\n",
      "[36/84] Processing: 4959777\n",
      "[37/84] Processing: 5153278\n",
      "[38/84] Processing: 4999301\n",
      "[39/84] Processing: 5118470\n",
      "[40/84] Processing: 4901100\n",
      "[41/84] Processing: 5115965\n",
      "[42/84] Processing: 4158518\n",
      "[43/84] Processing: 4884485\n",
      "[44/84] Processing: 5068771\n",
      "[45/84] Processing: 4891042\n",
      "[46/84] Processing: 5060163\n",
      "[47/84] Processing: 5107114\n",
      "[48/84] Processing: 3818945\n",
      "[49/84] Processing: 5144278\n",
      "[50/84] Processing: 4486588\n",
      "[51/84] Processing: 4280166\n",
      "[52/84] Processing: 5110014\n",
      "[53/84] Processing: 5112825\n",
      "[54/84] Processing: 4632009\n",
      "[55/84] Processing: 5181767\n",
      "[56/84] Processing: 5089925\n",
      "[57/84] Processing: 5103590\n",
      "[58/84] Processing: 4983218\n",
      "[59/84] Processing: 5187275\n",
      "[60/84] Processing: 3713070\n",
      "[61/84] Processing: 4038124\n",
      "[62/84] Processing: 5167908\n",
      "[63/84] Processing: 4266689\n",
      "[64/84] Processing: 5181362\n",
      "[65/84] Processing: 5186086\n",
      "[66/84] Processing: 5184523\n",
      "[67/84] Processing: 5168065\n",
      "[68/84] Processing: 5163744\n",
      "[69/84] Processing: 5187276\n",
      "[70/84] Processing: 5186450\n",
      "[71/84] Processing: 5187274\n",
      "[72/84] Processing: 5184531\n",
      "[73/84] Processing: 5179247\n",
      "[74/84] Processing: 4194193\n",
      "[75/84] Processing: 5149274\n",
      "[76/84] Processing: 5147145\n",
      "[77/84] Processing: 5095087\n",
      "[78/84] Processing: 5162858\n",
      "[79/84] Processing: 5145673\n",
      "[80/84] Processing: 5184396\n",
      "[81/84] Processing: 5084859\n",
      "[82/84] Processing: 5119065\n",
      "[83/84] Processing: 5161334\n",
      "[84/84] Processing: 5038503\n",
      "Scraping completed. Data saved to data/items_data.json\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0'\n",
    "}\n",
    "\n",
    "# Assume links is defined from previous scraping step (e.g., from get_links_from_base())\n",
    "# Example: links = [\"/items/5160924\", \"/items/5110023\", ...]\n",
    "\n",
    "def extract_item_data(link):\n",
    "    url = ITEM_URL_BASE + link\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract item ID\n",
    "    item_id = link.split(\"/\")[-1]\n",
    "\n",
    "    # Extract amount and currency using meta tags\n",
    "    amount_tag = soup.find('meta', {'property': 'product:price:amount'})\n",
    "    currency_tag = soup.find('meta', {'property': 'product:price:currency'})\n",
    "\n",
    "    amount = amount_tag['content'] if amount_tag else None\n",
    "    currency = currency_tag['content'] if currency_tag else None\n",
    "\n",
    "    # Extract property fields dynamically\n",
    "    properties = {}\n",
    "    prop_div = soup.find(\"div\", class_=\"product-properties__column\")\n",
    "    if prop_div:\n",
    "        for prop in prop_div.find_all(\"div\", class_=\"product-properties__i\"):\n",
    "            name = prop.find(\"label\", class_=\"product-properties__i-name\").get_text(strip=True)\n",
    "            value = prop.find(\"span\", class_=\"product-properties__i-value\").get_text(strip=True)\n",
    "            properties[name] = value\n",
    "\n",
    "    # Construct result dictionary\n",
    "    item_data = {\n",
    "        'item_id': item_id,\n",
    "        'amount': amount,\n",
    "        'currency': currency,\n",
    "    }\n",
    "\n",
    "    # Add all found properties dynamically\n",
    "    item_data.update(properties)\n",
    "\n",
    "    return item_data\n",
    "\n",
    "\n",
    "def scrape_all_items():\n",
    "    results = []\n",
    "\n",
    "    for idx, link in enumerate(links):\n",
    "        print(f\"[{idx+1}/{len(links)}] Processing: {link}\")\n",
    "        data = extract_item_data(link)\n",
    "        if data:\n",
    "            results.append(data)\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Scraping completed. Data saved to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "# Run the scraper\n",
    "scrape_all_items()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
