{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b836e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5112825', '4983218', '4113285', '4711907', '4038124', '5167908', '4687575', '4266689', '4839041', '5107114', '5172433', '5037112', '5184523', '5168065', '5184519', '5186450', '4624278', '4530215', '5179247', '5170034', '5181371', '5145673', '4999301', '5160637', '5068771', '5119065', '4632009', '5144278']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "ITEM_URL_BASE = \"https://bina.az/items/\"\n",
    "BASE_URL = \"https://bina.az/items/all\"\n",
    "HOME_FILE = \"data/test.html\"\n",
    "OUTPUT_FILE = \"data/items_data.json\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "def get_unique_item_ids():\n",
    "    # Send a request to the base URL\n",
    "    response = requests.get(BASE_URL, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page: {response.status_code}\")\n",
    "        return set()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    item_ids = set()\n",
    "    \n",
    "    # Find all item links and extract IDs from hrefs\n",
    "    for a in soup.select(\"a.item_link\"):  # Adjust selector if needed\n",
    "        href = a.get(\"href\")\n",
    "        if href:\n",
    "            # Use regex to extract numeric ID from /items/123456 format\n",
    "            match = re.search(r'/items/(\\d+)', href)\n",
    "            if match:\n",
    "                item_id = match.group(1)\n",
    "                item_ids.add(item_id)\n",
    "\n",
    "    # Optional: Save raw HTML for debugging\n",
    "    with open(HOME_FILE, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    return item_ids\n",
    "\n",
    "# Run the function and print results\n",
    "item_ids = get_unique_item_ids()\n",
    "links = list(item_ids)\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57aa24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/28] Processing: 5112825\n",
      "[2/28] Processing: 4983218\n",
      "[3/28] Processing: 4113285\n",
      "[4/28] Processing: 4711907\n",
      "[5/28] Processing: 4038124\n",
      "[6/28] Processing: 5167908\n",
      "[7/28] Processing: 4687575\n",
      "[8/28] Processing: 4266689\n",
      "[9/28] Processing: 4839041\n",
      "[10/28] Processing: 5107114\n",
      "[11/28] Processing: 5172433\n",
      "[12/28] Processing: 5037112\n",
      "[13/28] Processing: 5184523\n",
      "[14/28] Processing: 5168065\n",
      "[15/28] Processing: 5184519\n",
      "[16/28] Processing: 5186450\n",
      "[17/28] Processing: 4624278\n",
      "[18/28] Processing: 4530215\n",
      "[19/28] Processing: 5179247\n",
      "[20/28] Processing: 5170034\n",
      "[21/28] Processing: 5181371\n",
      "[22/28] Processing: 5145673\n",
      "[23/28] Processing: 4999301\n",
      "[24/28] Processing: 5160637\n",
      "[25/28] Processing: 5068771\n",
      "[26/28] Processing: 5119065\n",
      "[27/28] Processing: 4632009\n",
      "[28/28] Processing: 5144278\n",
      "Scraping completed. Data saved to data/items_data.json\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0'\n",
    "}\n",
    "\n",
    "# Assume links is defined from previous scraping step (e.g., from get_links_from_base())\n",
    "# Example: links = [\"/items/5160924\", \"/items/5110023\", ...]\n",
    "\n",
    "def extract_item_data(link):\n",
    "    url = ITEM_URL_BASE + link\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract item ID\n",
    "    item_id = link.split(\"/\")[-1]\n",
    "\n",
    "    # Extract amount and currency using meta tags\n",
    "    amount_tag = soup.find('meta', {'property': 'product:price:amount'})\n",
    "    currency_tag = soup.find('meta', {'property': 'product:price:currency'})\n",
    "\n",
    "    amount = amount_tag['content'] if amount_tag else None\n",
    "    currency = currency_tag['content'] if currency_tag else None\n",
    "\n",
    "    # Extract property fields dynamically\n",
    "    properties = {}\n",
    "    prop_div = soup.find(\"div\", class_=\"product-properties__column\")\n",
    "    if prop_div:\n",
    "        for prop in prop_div.find_all(\"div\", class_=\"product-properties__i\"):\n",
    "            name = prop.find(\"label\", class_=\"product-properties__i-name\").get_text(strip=True)\n",
    "            value = prop.find(\"span\", class_=\"product-properties__i-value\").get_text(strip=True)\n",
    "            properties[name] = value\n",
    "\n",
    "    # Construct result dictionary\n",
    "    item_data = {\n",
    "        'item_id': item_id,\n",
    "        'amount': amount,\n",
    "        'currency': currency,\n",
    "    }\n",
    "\n",
    "    # Add all found properties dynamically\n",
    "    item_data.update(properties)\n",
    "\n",
    "    return item_data\n",
    "\n",
    "\n",
    "def scrape_all_items():\n",
    "    results = []\n",
    "\n",
    "    for idx, link in enumerate(links):\n",
    "        print(f\"[{idx+1}/{len(links)}] Processing: {link}\")\n",
    "        data = extract_item_data(link)\n",
    "        if data:\n",
    "            results.append(data)\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Scraping completed. Data saved to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "# Run the scraper\n",
    "scrape_all_items()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
