{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b836e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 02:32:50 - scraper - INFO - Starting to scrape item IDs (max pages: 3)\n",
      "2025-05-09 02:32:51 - scraper - INFO - Found 28 new items on page 1\n",
      "2025-05-09 02:32:51 - scraper - INFO - Found 28 new items on page 2\n",
      "2025-05-09 02:32:52 - scraper - INFO - Found 28 new items on page 3\n",
      "2025-05-09 02:32:52 - scraper - INFO - Reached max pages limit (3)\n",
      "2025-05-09 02:32:52 - scraper - INFO - Finished scraping. Found 84 unique items\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5056827', '4959734', '5142259', '5165167', '5122759', '4802446', '5141958', '3852754', '5153399', '2716163', '5186675', '5169209', '4351437', '4939762', '5166148', '4912143', '5172433', '4617223', '4749417', '5094015', '5169083', '3533767', '4986500', '5165139', '4515580', '4881242', '5038500', '4624278', '5136085', '5097244', '5170034', '4908448', '5063184', '5111890', '4822793', '4959777', '5153278', '4999301', '4901100', '5153700', '5115965', '4884485', '4234996', '4965109', '4891042', '5060163', '3161506', '4280166', '5073997', '5032165', '5136086', '5166133', '5110014', '5181767', '5089925', '4983218', '3713070', '5064932', '5094044', '5176057', '4840131', '5058889', '5181362', '5158541', '5103258', '5163744', '5129119', '4997082', '5186450', '5174093', '5184531', '5147145', '5003954', '5095087', '5156675', '3309447', '5178713', '5184396', '5084859', '5140712', '5149053', '5161334', '5038503', '5126935']\n",
      "84\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from config.config import ITEM_URL_BASE, BASE_URL, HOME_FILE, OUTPUT_FILE\n",
    "from logger.logger import logger  # Import the configured logger\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "def get_unique_item_ids(max_pages=3):\n",
    "    \"\"\"Fetch unique item IDs from paginated pages with proper logging.\"\"\"\n",
    "    item_ids = set()\n",
    "    page = 1\n",
    "    has_more_pages = True\n",
    "    \n",
    "    logger.info(f\"Starting to scrape item IDs (max pages: {max_pages})\")\n",
    "    \n",
    "    while has_more_pages:\n",
    "        if max_pages and page > max_pages:\n",
    "            logger.info(f\"Reached max pages limit ({max_pages})\")\n",
    "            break\n",
    "            \n",
    "        url = f\"{BASE_URL}?page={page}\"\n",
    "        logger.debug(f\"Fetching page {page}: {url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                logger.error(f\"Failed to fetch page {page}. Status code: {response.status_code}\")\n",
    "                break\n",
    "\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Find all item links and extract IDs from hrefs\n",
    "            current_page_ids = set()\n",
    "            for a in soup.select(\"a.item_link\"):\n",
    "                href = a.get(\"href\")\n",
    "                if href:\n",
    "                    match = re.search(r'/items/(\\d+)', href)\n",
    "                    if match:\n",
    "                        item_id = match.group(1)\n",
    "                        current_page_ids.add(item_id)\n",
    "            \n",
    "            # Check if we found any new items on this page\n",
    "            if not current_page_ids:\n",
    "                logger.info(f\"No items found on page {page}, stopping pagination\")\n",
    "                has_more_pages = False\n",
    "            else:\n",
    "                new_items = current_page_ids - item_ids\n",
    "                if not new_items:\n",
    "                    logger.info(f\"No new items found on page {page}, stopping pagination\")\n",
    "                    has_more_pages = False\n",
    "                else:\n",
    "                    logger.info(f\"Found {len(new_items)} new items on page {page}\")\n",
    "                    item_ids.update(current_page_ids)\n",
    "                    page += 1\n",
    "                    \n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"Network error fetching page {page}: {str(e)}\", exc_info=True)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error processing page {page}: {str(e)}\", exc_info=True)\n",
    "            break\n",
    "\n",
    "    # Save raw HTML for debugging\n",
    "    try:\n",
    "        with open(HOME_FILE, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            f.write(response.text)\n",
    "        logger.debug(f\"Saved last page HTML to {HOME_FILE}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save HTML to {HOME_FILE}: {str(e)}\", exc_info=True)\n",
    "\n",
    "    logger.info(f\"Finished scraping. Found {len(item_ids)} unique items\")\n",
    "    return item_ids\n",
    "\n",
    "# Run the function and print results\n",
    "item_ids = get_unique_item_ids()\n",
    "links = list(item_ids)\n",
    "print(links)\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57aa24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 02:34:57 - scraper - INFO - Starting to process 84 items\n",
      "2025-05-09 02:34:57 - scraper - INFO - [1/84] Processing: 5056827\n",
      "2025-05-09 02:34:57 - scraper - INFO - Successfully processed item: 5056827\n",
      "2025-05-09 02:34:57 - scraper - INFO - [2/84] Processing: 4959734\n",
      "2025-05-09 02:34:58 - scraper - INFO - Successfully processed item: 4959734\n",
      "2025-05-09 02:34:58 - scraper - INFO - [3/84] Processing: 5142259\n",
      "2025-05-09 02:34:59 - scraper - INFO - Successfully processed item: 5142259\n",
      "2025-05-09 02:34:59 - scraper - INFO - [4/84] Processing: 5165167\n",
      "2025-05-09 02:35:00 - scraper - INFO - Successfully processed item: 5165167\n",
      "2025-05-09 02:35:00 - scraper - INFO - [5/84] Processing: 5122759\n",
      "2025-05-09 02:35:00 - scraper - INFO - Successfully processed item: 5122759\n",
      "2025-05-09 02:35:00 - scraper - INFO - [6/84] Processing: 4802446\n",
      "2025-05-09 02:35:01 - scraper - INFO - Successfully processed item: 4802446\n",
      "2025-05-09 02:35:01 - scraper - INFO - [7/84] Processing: 5141958\n",
      "2025-05-09 02:35:02 - scraper - INFO - Successfully processed item: 5141958\n",
      "2025-05-09 02:35:02 - scraper - INFO - [8/84] Processing: 3852754\n",
      "2025-05-09 02:35:03 - scraper - INFO - Successfully processed item: 3852754\n",
      "2025-05-09 02:35:03 - scraper - INFO - [9/84] Processing: 5153399\n",
      "2025-05-09 02:35:04 - scraper - INFO - Successfully processed item: 5153399\n",
      "2025-05-09 02:35:04 - scraper - INFO - [10/84] Processing: 2716163\n",
      "2025-05-09 02:35:05 - scraper - INFO - Successfully processed item: 2716163\n",
      "2025-05-09 02:35:05 - scraper - INFO - [11/84] Processing: 5186675\n",
      "2025-05-09 02:35:05 - scraper - INFO - Successfully processed item: 5186675\n",
      "2025-05-09 02:35:05 - scraper - INFO - [12/84] Processing: 5169209\n",
      "2025-05-09 02:35:06 - scraper - INFO - Successfully processed item: 5169209\n",
      "2025-05-09 02:35:06 - scraper - INFO - [13/84] Processing: 4351437\n",
      "2025-05-09 02:35:07 - scraper - INFO - Successfully processed item: 4351437\n",
      "2025-05-09 02:35:07 - scraper - INFO - [14/84] Processing: 4939762\n",
      "2025-05-09 02:35:07 - scraper - INFO - Successfully processed item: 4939762\n",
      "2025-05-09 02:35:07 - scraper - INFO - [15/84] Processing: 5166148\n",
      "2025-05-09 02:35:08 - scraper - INFO - Successfully processed item: 5166148\n",
      "2025-05-09 02:35:08 - scraper - INFO - [16/84] Processing: 4912143\n",
      "2025-05-09 02:35:09 - scraper - INFO - Successfully processed item: 4912143\n",
      "2025-05-09 02:35:09 - scraper - INFO - [17/84] Processing: 5172433\n",
      "2025-05-09 02:35:10 - scraper - INFO - Successfully processed item: 5172433\n",
      "2025-05-09 02:35:10 - scraper - INFO - [18/84] Processing: 4617223\n",
      "2025-05-09 02:35:10 - scraper - INFO - Successfully processed item: 4617223\n",
      "2025-05-09 02:35:10 - scraper - INFO - [19/84] Processing: 4749417\n",
      "2025-05-09 02:35:11 - scraper - INFO - Successfully processed item: 4749417\n",
      "2025-05-09 02:35:11 - scraper - INFO - [20/84] Processing: 5094015\n",
      "2025-05-09 02:35:12 - scraper - INFO - Successfully processed item: 5094015\n",
      "2025-05-09 02:35:12 - scraper - INFO - [21/84] Processing: 5169083\n",
      "2025-05-09 02:35:13 - scraper - INFO - Successfully processed item: 5169083\n",
      "2025-05-09 02:35:13 - scraper - INFO - [22/84] Processing: 3533767\n",
      "2025-05-09 02:35:13 - scraper - INFO - Successfully processed item: 3533767\n",
      "2025-05-09 02:35:13 - scraper - INFO - [23/84] Processing: 4986500\n",
      "2025-05-09 02:35:14 - scraper - INFO - Successfully processed item: 4986500\n",
      "2025-05-09 02:35:14 - scraper - INFO - [24/84] Processing: 5165139\n",
      "2025-05-09 02:35:15 - scraper - INFO - Successfully processed item: 5165139\n",
      "2025-05-09 02:35:15 - scraper - INFO - [25/84] Processing: 4515580\n",
      "2025-05-09 02:35:16 - scraper - INFO - Successfully processed item: 4515580\n",
      "2025-05-09 02:35:16 - scraper - INFO - [26/84] Processing: 4881242\n",
      "2025-05-09 02:35:16 - scraper - INFO - Successfully processed item: 4881242\n",
      "2025-05-09 02:35:16 - scraper - INFO - [27/84] Processing: 5038500\n",
      "2025-05-09 02:35:17 - scraper - INFO - Successfully processed item: 5038500\n",
      "2025-05-09 02:35:17 - scraper - INFO - [28/84] Processing: 4624278\n",
      "2025-05-09 02:35:17 - scraper - INFO - Successfully processed item: 4624278\n",
      "2025-05-09 02:35:17 - scraper - INFO - [29/84] Processing: 5136085\n",
      "2025-05-09 02:35:18 - scraper - INFO - Successfully processed item: 5136085\n",
      "2025-05-09 02:35:18 - scraper - INFO - [30/84] Processing: 5097244\n",
      "2025-05-09 02:35:19 - scraper - INFO - Successfully processed item: 5097244\n",
      "2025-05-09 02:35:19 - scraper - INFO - [31/84] Processing: 5170034\n",
      "2025-05-09 02:35:19 - scraper - INFO - Successfully processed item: 5170034\n",
      "2025-05-09 02:35:19 - scraper - INFO - [32/84] Processing: 4908448\n",
      "2025-05-09 02:35:20 - scraper - INFO - Successfully processed item: 4908448\n",
      "2025-05-09 02:35:20 - scraper - INFO - [33/84] Processing: 5063184\n",
      "2025-05-09 02:35:21 - scraper - INFO - Successfully processed item: 5063184\n",
      "2025-05-09 02:35:21 - scraper - INFO - [34/84] Processing: 5111890\n",
      "2025-05-09 02:35:22 - scraper - INFO - Successfully processed item: 5111890\n",
      "2025-05-09 02:35:22 - scraper - INFO - [35/84] Processing: 4822793\n",
      "2025-05-09 02:35:22 - scraper - INFO - Successfully processed item: 4822793\n",
      "2025-05-09 02:35:22 - scraper - INFO - [36/84] Processing: 4959777\n",
      "2025-05-09 02:35:23 - scraper - INFO - Successfully processed item: 4959777\n",
      "2025-05-09 02:35:23 - scraper - INFO - [37/84] Processing: 5153278\n",
      "2025-05-09 02:35:24 - scraper - INFO - Successfully processed item: 5153278\n",
      "2025-05-09 02:35:24 - scraper - INFO - [38/84] Processing: 4999301\n",
      "2025-05-09 02:35:25 - scraper - INFO - Successfully processed item: 4999301\n",
      "2025-05-09 02:35:25 - scraper - INFO - [39/84] Processing: 4901100\n",
      "2025-05-09 02:35:25 - scraper - INFO - Successfully processed item: 4901100\n",
      "2025-05-09 02:35:25 - scraper - INFO - [40/84] Processing: 5153700\n",
      "2025-05-09 02:35:26 - scraper - INFO - Successfully processed item: 5153700\n",
      "2025-05-09 02:35:26 - scraper - INFO - [41/84] Processing: 5115965\n",
      "2025-05-09 02:35:27 - scraper - INFO - Successfully processed item: 5115965\n",
      "2025-05-09 02:35:27 - scraper - INFO - [42/84] Processing: 4884485\n",
      "2025-05-09 02:35:28 - scraper - INFO - Successfully processed item: 4884485\n",
      "2025-05-09 02:35:28 - scraper - INFO - [43/84] Processing: 4234996\n",
      "2025-05-09 02:35:29 - scraper - INFO - Successfully processed item: 4234996\n",
      "2025-05-09 02:35:29 - scraper - INFO - [44/84] Processing: 4965109\n",
      "2025-05-09 02:35:29 - scraper - INFO - Successfully processed item: 4965109\n",
      "2025-05-09 02:35:29 - scraper - INFO - [45/84] Processing: 4891042\n",
      "2025-05-09 02:35:30 - scraper - INFO - Successfully processed item: 4891042\n",
      "2025-05-09 02:35:30 - scraper - INFO - [46/84] Processing: 5060163\n",
      "2025-05-09 02:35:31 - scraper - INFO - Successfully processed item: 5060163\n",
      "2025-05-09 02:35:31 - scraper - INFO - [47/84] Processing: 3161506\n",
      "2025-05-09 02:35:32 - scraper - INFO - Successfully processed item: 3161506\n",
      "2025-05-09 02:35:32 - scraper - INFO - [48/84] Processing: 4280166\n",
      "2025-05-09 02:35:33 - scraper - INFO - Successfully processed item: 4280166\n",
      "2025-05-09 02:35:33 - scraper - INFO - [49/84] Processing: 5073997\n",
      "2025-05-09 02:35:34 - scraper - INFO - Successfully processed item: 5073997\n",
      "2025-05-09 02:35:34 - scraper - INFO - [50/84] Processing: 5032165\n",
      "2025-05-09 02:35:34 - scraper - INFO - Successfully processed item: 5032165\n",
      "2025-05-09 02:35:34 - scraper - INFO - [51/84] Processing: 5136086\n",
      "2025-05-09 02:35:35 - scraper - INFO - Successfully processed item: 5136086\n",
      "2025-05-09 02:35:35 - scraper - INFO - [52/84] Processing: 5166133\n",
      "2025-05-09 02:35:35 - scraper - INFO - Successfully processed item: 5166133\n",
      "2025-05-09 02:35:35 - scraper - INFO - [53/84] Processing: 5110014\n",
      "2025-05-09 02:35:36 - scraper - INFO - Successfully processed item: 5110014\n",
      "2025-05-09 02:35:36 - scraper - INFO - [54/84] Processing: 5181767\n",
      "2025-05-09 02:35:37 - scraper - INFO - Successfully processed item: 5181767\n",
      "2025-05-09 02:35:37 - scraper - INFO - [55/84] Processing: 5089925\n",
      "2025-05-09 02:35:38 - scraper - INFO - Successfully processed item: 5089925\n",
      "2025-05-09 02:35:38 - scraper - INFO - [56/84] Processing: 4983218\n",
      "2025-05-09 02:35:38 - scraper - INFO - Successfully processed item: 4983218\n",
      "2025-05-09 02:35:38 - scraper - INFO - [57/84] Processing: 3713070\n",
      "2025-05-09 02:35:39 - scraper - INFO - Successfully processed item: 3713070\n",
      "2025-05-09 02:35:39 - scraper - INFO - [58/84] Processing: 5064932\n",
      "2025-05-09 02:35:40 - scraper - INFO - Successfully processed item: 5064932\n",
      "2025-05-09 02:35:40 - scraper - INFO - [59/84] Processing: 5094044\n",
      "2025-05-09 02:35:41 - scraper - INFO - Successfully processed item: 5094044\n",
      "2025-05-09 02:35:41 - scraper - INFO - [60/84] Processing: 5176057\n",
      "2025-05-09 02:35:43 - scraper - INFO - Successfully processed item: 5176057\n",
      "2025-05-09 02:35:43 - scraper - INFO - [61/84] Processing: 4840131\n",
      "2025-05-09 02:35:43 - scraper - INFO - Successfully processed item: 4840131\n",
      "2025-05-09 02:35:43 - scraper - INFO - [62/84] Processing: 5058889\n",
      "2025-05-09 02:35:44 - scraper - INFO - Successfully processed item: 5058889\n",
      "2025-05-09 02:35:44 - scraper - INFO - [63/84] Processing: 5181362\n",
      "2025-05-09 02:35:45 - scraper - INFO - Successfully processed item: 5181362\n",
      "2025-05-09 02:35:45 - scraper - INFO - [64/84] Processing: 5158541\n",
      "2025-05-09 02:35:45 - scraper - INFO - Successfully processed item: 5158541\n",
      "2025-05-09 02:35:45 - scraper - INFO - [65/84] Processing: 5103258\n",
      "2025-05-09 02:35:46 - scraper - INFO - Successfully processed item: 5103258\n",
      "2025-05-09 02:35:46 - scraper - INFO - [66/84] Processing: 5163744\n",
      "2025-05-09 02:35:47 - scraper - INFO - Successfully processed item: 5163744\n",
      "2025-05-09 02:35:47 - scraper - INFO - [67/84] Processing: 5129119\n",
      "2025-05-09 02:35:47 - scraper - INFO - Successfully processed item: 5129119\n",
      "2025-05-09 02:35:47 - scraper - INFO - [68/84] Processing: 4997082\n",
      "2025-05-09 02:35:48 - scraper - INFO - Successfully processed item: 4997082\n",
      "2025-05-09 02:35:48 - scraper - INFO - [69/84] Processing: 5186450\n",
      "2025-05-09 02:35:49 - scraper - INFO - Successfully processed item: 5186450\n",
      "2025-05-09 02:35:49 - scraper - INFO - [70/84] Processing: 5174093\n",
      "2025-05-09 02:35:49 - scraper - INFO - Successfully processed item: 5174093\n",
      "2025-05-09 02:35:49 - scraper - INFO - [71/84] Processing: 5184531\n",
      "2025-05-09 02:35:50 - scraper - INFO - Successfully processed item: 5184531\n",
      "2025-05-09 02:35:50 - scraper - INFO - [72/84] Processing: 5147145\n",
      "2025-05-09 02:35:50 - scraper - INFO - Successfully processed item: 5147145\n",
      "2025-05-09 02:35:50 - scraper - INFO - [73/84] Processing: 5003954\n",
      "2025-05-09 02:35:51 - scraper - INFO - Successfully processed item: 5003954\n",
      "2025-05-09 02:35:51 - scraper - INFO - [74/84] Processing: 5095087\n",
      "2025-05-09 02:35:52 - scraper - INFO - Successfully processed item: 5095087\n",
      "2025-05-09 02:35:52 - scraper - INFO - [75/84] Processing: 5156675\n",
      "2025-05-09 02:35:52 - scraper - INFO - Successfully processed item: 5156675\n",
      "2025-05-09 02:35:52 - scraper - INFO - [76/84] Processing: 3309447\n",
      "2025-05-09 02:35:53 - scraper - INFO - Successfully processed item: 3309447\n",
      "2025-05-09 02:35:53 - scraper - INFO - [77/84] Processing: 5178713\n",
      "2025-05-09 02:35:54 - scraper - INFO - Successfully processed item: 5178713\n",
      "2025-05-09 02:35:54 - scraper - INFO - [78/84] Processing: 5184396\n",
      "2025-05-09 02:35:55 - scraper - INFO - Successfully processed item: 5184396\n",
      "2025-05-09 02:35:55 - scraper - INFO - [79/84] Processing: 5084859\n",
      "2025-05-09 02:35:56 - scraper - INFO - Successfully processed item: 5084859\n",
      "2025-05-09 02:35:56 - scraper - INFO - [80/84] Processing: 5140712\n",
      "2025-05-09 02:35:57 - scraper - INFO - Successfully processed item: 5140712\n",
      "2025-05-09 02:35:57 - scraper - INFO - [81/84] Processing: 5149053\n",
      "2025-05-09 02:35:57 - scraper - INFO - Successfully processed item: 5149053\n",
      "2025-05-09 02:35:57 - scraper - INFO - [82/84] Processing: 5161334\n",
      "2025-05-09 02:35:58 - scraper - INFO - Successfully processed item: 5161334\n",
      "2025-05-09 02:35:58 - scraper - INFO - [83/84] Processing: 5038503\n",
      "2025-05-09 02:35:59 - scraper - INFO - Successfully processed item: 5038503\n",
      "2025-05-09 02:35:59 - scraper - INFO - [84/84] Processing: 5126935\n",
      "2025-05-09 02:35:59 - scraper - INFO - Successfully processed item: 5126935\n",
      "2025-05-09 02:35:59 - scraper - INFO - Successfully saved 84 items to data/items_data.json\n"
     ]
    }
   ],
   "source": [
    "def extract_item_data(link):\n",
    "    \"\"\"Extract detailed item data from individual product page with comprehensive logging\"\"\"\n",
    "    url = ITEM_URL_BASE + link\n",
    "    logger.debug(f\"Starting to extract data from: {url}\")\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Failed to fetch {url} - Status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Extract item ID\n",
    "        item_id = link.split(\"/\")[-1]\n",
    "        logger.debug(f\"Processing item ID: {item_id}\")\n",
    "\n",
    "        # Extract amount and currency using meta tags\n",
    "        amount_tag = soup.find('meta', {'property': 'product:price:amount'})\n",
    "        currency_tag = soup.find('meta', {'property': 'product:price:currency'})\n",
    "\n",
    "        amount = amount_tag['content'] if amount_tag else None\n",
    "        currency = currency_tag['content'] if currency_tag else None\n",
    "        logger.debug(f\"Extracted price: {amount} {currency}\")\n",
    "\n",
    "        # Extract property fields dynamically\n",
    "        properties = {}\n",
    "        prop_div = soup.find(\"div\", class_=\"product-properties__column\")\n",
    "        if prop_div:\n",
    "            for prop in prop_div.find_all(\"div\", class_=\"product-properties__i\"):\n",
    "                try:\n",
    "                    name = prop.find(\"label\", class_=\"product-properties__i-name\").get_text(strip=True)\n",
    "                    value = prop.find(\"span\", class_=\"product-properties__i-value\").get_text(strip=True)\n",
    "                    properties[name] = value\n",
    "                    logger.debug(f\"Found property: {name} = {value}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to extract property: {str(e)}\", exc_info=True)\n",
    "\n",
    "        # Extract locations\n",
    "        location_tags = soup.find_all('a', {'data-stat': 'product-locations'})\n",
    "        locations = [tag.get_text(strip=True) for tag in location_tags] if location_tags else []\n",
    "        logger.debug(f\"Found locations: {locations}\")\n",
    "\n",
    "        # Construct result dictionary\n",
    "        item_data = {\n",
    "            'item_id': item_id,\n",
    "            'amount': amount,\n",
    "            'currency': currency,\n",
    "            'location': locations\n",
    "        }\n",
    "\n",
    "        # Add all found properties dynamically\n",
    "        item_data.update(properties)\n",
    "        \n",
    "        logger.info(f\"Successfully processed item: {item_id}\")\n",
    "        return item_data\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        logger.error(f\"Network error while processing {url}: {str(e)}\", exc_info=True)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error processing {url}: {str(e)}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "\n",
    "def scrape_all_items():\n",
    "    \"\"\"Main scraping function that processes all items with progress tracking\"\"\"\n",
    "    if not links:\n",
    "        logger.warning(\"No links to process - empty list provided\")\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    logger.info(f\"Starting to process {len(links)} items\")\n",
    "\n",
    "    for idx, link in enumerate(links, 1):\n",
    "        logger.info(f\"[{idx}/{len(links)}] Processing: {link}\")\n",
    "        data = extract_item_data(link)\n",
    "        if data:\n",
    "            results.append(data)\n",
    "            logger.debug(f\"Added item {data.get('item_id')} to results\")\n",
    "        else:\n",
    "            logger.warning(f\"Failed to process item: {link}\")\n",
    "\n",
    "    # Save to JSON\n",
    "    try:\n",
    "        with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Successfully saved {len(results)} items to {OUTPUT_FILE}\")\n",
    "        if len(results) != len(links):\n",
    "            logger.warning(f\"Processed {len(results)} out of {len(links)} items ({(len(results)/len(links))*100:.1f}% success rate)\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save data to {OUTPUT_FILE}: {str(e)}\", exc_info=True)\n",
    "\n",
    "\n",
    "# Run the scraper\n",
    "scrape_all_items()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
