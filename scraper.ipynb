{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b836e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/items/4347196', '/items/4347196', '/items/5114087', '/items/5114087', '/items/5044379', '/items/5044379', '/items/5044379', '/items/5044379', '/items/5168196', '/items/5168196', '/items/5168196', '/items/5168196', '/items/5064125', '/items/5064125', '/items/5064125', '/items/5187268', '/items/5187268', '/items/4759000', '/items/4759000', '/items/4759000', '/items/5178543', '/items/5178543', '/items/5178543', '/items/5178543', '/items/5119747', '/items/5119747', '/items/5119747', '/items/5119747', '/items/5187267', '/items/5187267', '/items/5187267', '/items/5187267', '/items/5010698', '/items/5010698', '/items/5010698', '/items/5109891', '/items/5109891', '/items/5109891', '/items/5109891', '/items/4618218', '/items/4618218', '/items/4618218', '/items/5187266', '/items/5187266', '/items/5187266', '/items/5156030', '/items/5156030', '/items/5120898', '/items/5120898', '/items/5120898', '/items/5056395', '/items/5056395', '/items/5056395', '/items/5056395', '/items/4844105', '/items/5110435', '/items/5110435', '/items/5110435', '/items/5110435', '/items/5187262', '/items/5187262', '/items/5187262', '/items/4507519', '/items/4507519', '/items/5136074', '/items/5136074', '/items/5136074', '/items/5136074', '/items/5147584', '/items/5147584', '/items/5187265', '/items/5187265', '/items/5187264', '/items/5187264', '/items/5180203', '/items/5180203', '/items/5187263', '/items/5187263', '/items/5165713', '/items/5165713', '/items/5165713']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "\n",
    "ITEM_URL_BASE = \"https://bina.az\"\n",
    "BASE_URL = \"https://bina.az/items/all\"\n",
    "ITEM_FILE = \"data/item.html\"\n",
    "HOME_FILE = \"data/test.html\"\n",
    "OUTPUT_FILE = \"data/items_data.json\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "\n",
    "def get_links_from_base():\n",
    "    # Send a request to the base URL\n",
    "    response = requests.get(BASE_URL, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch page: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    links = []\n",
    "    # Select all ad links from the page\n",
    "    for a in soup.select(\"a.item_link\"):  # Use correct class name here\n",
    "        href = a.get(\"href\")\n",
    "        if href:\n",
    "            links.append(href)\n",
    "\n",
    "    # Optional: Save raw HTML to file for debugging or future use\n",
    "    with open(HOME_FILE, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        f.write(response.text)\n",
    "\n",
    "    return links\n",
    "\n",
    "# Run the function and print results\n",
    "links = get_links_from_base()\n",
    "print(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57aa24ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/81] Processing: /items/4347196\n",
      "[2/81] Processing: /items/4347196\n",
      "[3/81] Processing: /items/5114087\n",
      "[4/81] Processing: /items/5114087\n",
      "[5/81] Processing: /items/5044379\n",
      "[6/81] Processing: /items/5044379\n",
      "[7/81] Processing: /items/5044379\n",
      "[8/81] Processing: /items/5044379\n",
      "[9/81] Processing: /items/5168196\n",
      "[10/81] Processing: /items/5168196\n",
      "[11/81] Processing: /items/5168196\n",
      "[12/81] Processing: /items/5168196\n",
      "[13/81] Processing: /items/5064125\n",
      "[14/81] Processing: /items/5064125\n",
      "[15/81] Processing: /items/5064125\n",
      "[16/81] Processing: /items/5187268\n",
      "[17/81] Processing: /items/5187268\n",
      "[18/81] Processing: /items/4759000\n",
      "[19/81] Processing: /items/4759000\n",
      "[20/81] Processing: /items/4759000\n",
      "[21/81] Processing: /items/5178543\n",
      "[22/81] Processing: /items/5178543\n",
      "[23/81] Processing: /items/5178543\n",
      "[24/81] Processing: /items/5178543\n",
      "[25/81] Processing: /items/5119747\n",
      "[26/81] Processing: /items/5119747\n",
      "[27/81] Processing: /items/5119747\n",
      "[28/81] Processing: /items/5119747\n",
      "[29/81] Processing: /items/5187267\n",
      "[30/81] Processing: /items/5187267\n",
      "[31/81] Processing: /items/5187267\n",
      "[32/81] Processing: /items/5187267\n",
      "[33/81] Processing: /items/5010698\n",
      "[34/81] Processing: /items/5010698\n",
      "[35/81] Processing: /items/5010698\n",
      "[36/81] Processing: /items/5109891\n",
      "[37/81] Processing: /items/5109891\n",
      "[38/81] Processing: /items/5109891\n",
      "[39/81] Processing: /items/5109891\n",
      "[40/81] Processing: /items/4618218\n",
      "[41/81] Processing: /items/4618218\n",
      "[42/81] Processing: /items/4618218\n",
      "[43/81] Processing: /items/5187266\n",
      "[44/81] Processing: /items/5187266\n",
      "[45/81] Processing: /items/5187266\n",
      "[46/81] Processing: /items/5156030\n",
      "[47/81] Processing: /items/5156030\n",
      "[48/81] Processing: /items/5120898\n",
      "[49/81] Processing: /items/5120898\n",
      "[50/81] Processing: /items/5120898\n",
      "[51/81] Processing: /items/5056395\n",
      "[52/81] Processing: /items/5056395\n",
      "[53/81] Processing: /items/5056395\n",
      "[54/81] Processing: /items/5056395\n",
      "[55/81] Processing: /items/4844105\n",
      "[56/81] Processing: /items/5110435\n",
      "[57/81] Processing: /items/5110435\n",
      "[58/81] Processing: /items/5110435\n",
      "[59/81] Processing: /items/5110435\n",
      "[60/81] Processing: /items/5187262\n",
      "[61/81] Processing: /items/5187262\n",
      "[62/81] Processing: /items/5187262\n",
      "[63/81] Processing: /items/4507519\n",
      "[64/81] Processing: /items/4507519\n",
      "[65/81] Processing: /items/5136074\n",
      "[66/81] Processing: /items/5136074\n",
      "[67/81] Processing: /items/5136074\n",
      "[68/81] Processing: /items/5136074\n",
      "[69/81] Processing: /items/5147584\n",
      "[70/81] Processing: /items/5147584\n",
      "[71/81] Processing: /items/5187265\n",
      "[72/81] Processing: /items/5187265\n",
      "[73/81] Processing: /items/5187264\n",
      "[74/81] Processing: /items/5187264\n",
      "[75/81] Processing: /items/5180203\n",
      "[76/81] Processing: /items/5180203\n",
      "[77/81] Processing: /items/5187263\n",
      "[78/81] Processing: /items/5187263\n",
      "[79/81] Processing: /items/5165713\n",
      "[80/81] Processing: /items/5165713\n",
      "[81/81] Processing: /items/5165713\n",
      "Scraping completed. Data saved to data/items_data.json\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0'\n",
    "}\n",
    "\n",
    "# Assume links is defined from previous scraping step (e.g., from get_links_from_base())\n",
    "# Example: links = [\"/items/5160924\", \"/items/5110023\", ...]\n",
    "\n",
    "def extract_item_data(link):\n",
    "    url = ITEM_URL_BASE + link\n",
    "    response = requests.get(url, headers=headers)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to fetch {url}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extract item ID\n",
    "    item_id = link.split(\"/\")[-1]\n",
    "\n",
    "    # Extract amount and currency using meta tags\n",
    "    amount_tag = soup.find('meta', {'property': 'product:price:amount'})\n",
    "    currency_tag = soup.find('meta', {'property': 'product:price:currency'})\n",
    "\n",
    "    amount = amount_tag['content'] if amount_tag else None\n",
    "    currency = currency_tag['content'] if currency_tag else None\n",
    "\n",
    "    # Extract property fields dynamically\n",
    "    properties = {}\n",
    "    prop_div = soup.find(\"div\", class_=\"product-properties__column\")\n",
    "    if prop_div:\n",
    "        for prop in prop_div.find_all(\"div\", class_=\"product-properties__i\"):\n",
    "            name = prop.find(\"label\", class_=\"product-properties__i-name\").get_text(strip=True)\n",
    "            value = prop.find(\"span\", class_=\"product-properties__i-value\").get_text(strip=True)\n",
    "            properties[name] = value\n",
    "\n",
    "    # Construct result dictionary\n",
    "    item_data = {\n",
    "        'item_id': item_id,\n",
    "        'amount': amount,\n",
    "        'currency': currency,\n",
    "    }\n",
    "\n",
    "    # Add all found properties dynamically\n",
    "    item_data.update(properties)\n",
    "\n",
    "    return item_data\n",
    "\n",
    "\n",
    "def scrape_all_items():\n",
    "    results = []\n",
    "\n",
    "    for idx, link in enumerate(links):\n",
    "        print(f\"[{idx+1}/{len(links)}] Processing: {link}\")\n",
    "        data = extract_item_data(link)\n",
    "        if data:\n",
    "            results.append(data)\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Scraping completed. Data saved to {OUTPUT_FILE}\")\n",
    "\n",
    "\n",
    "# Run the scraper\n",
    "scrape_all_items()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
